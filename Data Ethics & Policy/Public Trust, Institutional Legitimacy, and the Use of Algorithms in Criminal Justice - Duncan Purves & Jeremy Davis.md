#LIS461 #Ethics #Philosophy

## Thesis

In this paper we raise a novel moral concern with algorithmic opacity, one that arises from the relationship between algorithmic opacity and the trustworthiness of criminal justice institutions. In particular, we argue that algorithmic opacity can undermine the trustworthiness of criminal justice institutions, which threatens their legitimacy. This problem can persist even when predictive algorithms satisfy reasonable standards of fairness and when they are more accurate than humans in the same domain (3). 

## Intro

### Quotes

- Courts and law enforcement agencies across the U.S., as well as certain other countries, employ predictive algorithms for a variety of purposes, including risk assessment in sentencing, predicting locations where crime is likely to occur, and identifying those likely to commit criminal acts in the near future (2)
- There is no consensus definition of algorithmic opacity, but we understand an algorithm to count as opaque to some degree just in case (a) facts about the contribution of any single feature of the world to the algorithm’s final determination cannot be easily accessed, either by the human decision-maker or by persons affected by the determination, or (b) the way that the algorithmic determination figures in decision-making cannot be easily understood (2).

## Trust and Trustworthiness

The authors examine an argument by Karen Jones on the "three-place" account of trustworthiness and whether it applies to the trust between an institution and a user, rather than two people (and a domain of interaction). 
Jones's argument seems to hold on most counts, but fails when taking account whether institutions have the capacity to hold trust on the account of not having a direct motivational relationship to the person in question. 
This leads them to create their own version of Jones's statement:
	**Rich institutional three-place trustworthiness**: An institution I is richly trustworthy with respect to a subject S in domain of interaction D, if and only if I is competent with respect to D, I is non-accidentally responsive to the fact that S is counting on I, were S to do so in D, such that I functions as counted on, and I provides adequate reason for S to believe that I is competent with respect to D and non-accidentally responsive to the fact that S is counting on I in D.

### Quotes

- If being trustworthy requires taking the fact of another’s dependence to be a reason to act as counted on, then being trustworthy seems to require the possession of a capacity to respond to facts as reasons (6).
- An institution’s trustworthiness is not merely a function of the aggregate trustworthiness of its individual members; the structures in place within the institution, which may exist independently of our assessments of any one individual’s trustworthiness, are also a significant factor in the trustworthiness of the institution (8). 

## Forms of Opacity

The authors decompose algorithm opacity into four forms:
1. Proprietary: algorithm code may not be made publicly available
2. Technical: understanding the code is a specialized skill that the general public does not possess
3. Fundamental: the decision procedures of machine learning algorithms resist interpretation
4. Implementation: secrecy is kept as a competitive advantage. 
It is concluded that predictive policing algorithms are clouded by all the above. 

### Quotes

- When algorithms used by the police or courts are protected from scrutiny on the
grounds that the data constitutes proprietary information, the result is a kind of opacity that not only frustrates the accused’s ability to verify that their treatment was fair and appropriate, but also weakens the general public’s ability to have faith in the deployment of such methods in society more broadly (12). 

## Opacity and Trustworthiness

This opacity in the the algorithms can lead to public distrust in the policing system, and due to said opacity, it is almost impossible to analyze these systems to conclude either no wrong-doing or malpractice at play. The authors also mention *compounding injustice*, the case that algorithms may mark a marginalized group due to uncontrollable

### Quotes

- It is not merely the absence of publicly available training or output data that makes investigation impossible. It is that conducting such an investigation requires knowledge of advanced statistical techniques that few possess. Thus, opacity enables discrimination masking (15). 
- If discrimination or unfairness can be addressed without overcoming opacity, then we have overstated the threat to trustworthiness that opacity poses. But addressing discrimination without overcoming opacity assumes that we can identify sources of unfairness simply by evaluating the outcomes an algorithmic system produces (16). 
- Discrimination, at least in part, depends upon treatment. That is, in addition to whether outcomes are unequal or suggestive of discrimination, it also matters whether individuals’ interests were not equally considered, discounted, or other relevant constraints were violated in the process leading to a determination. And, crucially, this is something that cannot be understood merely by looking at outcomes: we need to understand how the outcomes were produced, which requires access to the inner workings of the algorithm (16). 