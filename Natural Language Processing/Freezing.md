From the 2019 paper [What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning](https://arxiv.org/abs/1911.03090) by Lee et al. 

## Overview
Large pretrained models like BERT can take a long time and a lot of compute power to fine-tune, but 