
### Working Set
We've set up processes to have virtual memory, but how do we decide how much memory a process needs? 
We define the working set for a process at time $t$ as $W(D,t)$, where $D$ is a window of time and $W$ ends up being an approximation of the program's locality. We can give a process an amount of memory that is a function of the amount of pages it has used in the last time step (and if it's the first time the program has run, guess). 
This also works as a function of page faults. More page faults means we need more memory, and vice versa. 
All information related to the working set is stored in the PCB. 

If you take the curve relating the number of page frames to the page fault rate, you can find a point of inflection that acts as that approximation for the correct working set size. 

### Thrashing
If you run out of memory, you end up trashing pages directly after bringing them into memory. This is called thrashing, where most of your time is spent in page faults. The solution is often to start randomly killing processes using the most memory. 

### Swapping
The kernel can organize regions of VM as segments according to how they are swapped. This improves locality as data that gets swapped to the same file all goes to the same segment. Multiple memory areas can get swapped to the same file or partition, but anywhere in that partition. 
The swap daemon maintains a swap map that includes which blocks on disk are in use and which virtual pages are stored in said blocks. 

### Mapping
The kernel can use mapping to organize file permissions, such as allowing execute permissions through an offset. This also works for sharing if the kernel maps two processes to the same location that maps back to one file on disk. 

## Concurrency
Moore's law is dead, and now we just pile more cores into the same processor. This means we need a way to allow applications to use multiple cores at the same time. 
One option is building an app with separated logical processes, and using pipes to transfer the data between the process. However, this includes a lot of overhead with copying memory. Context switches are also more expensive since each process has it's own page table, which is more page faults. 

We'll introduce the concept of a thread. These have stacks and stack pointers, program counters, etc. However, multiple threads share the same address space, code, heap, PIDs, etc. Depending on the OS, these may run on one core or multiple cores. 
There are multiple ways to program with threads:
1. Producer / Consumer. Some nodes are producing, some are consuming. Think ROS2 or [[Kafka]]. 
2. Pipeline: The task is a series of subtasks, each of which is handled by its own thread. 
3. Defer work with background thread. Non-critical work is done in the background while the CPU is idle. 

To create threads, processes have options depending on their OS. One of those is POSIX Pthreads. 
Common thread operations include create, exit, and join, but there is no fork. 