
### Working Set
We've set up processes to have virtual memory, but how do we decide how much memory a process needs? 
We define the working set for a process at time $t$ as $W(D,t)$, where $D$ is a window of time and $W$ ends up being an approximation of the program's locality. We can give a process an amount of memory that is a function of the amount of pages it has used in the last time step (and if it's the first time the program has run, guess). 
This also works as a function of page faults. More page faults means we need more memory, and vice versa. 
All information related to the working set is stored in the PCB. 

If you take the curve relating the number of page frames to the page fault rate, you can find a point of inflection that acts as that approximation for the correct working set size. 

### Thrashing
If you run out of memory, you end up trashing pages directly after bringing them into memory. This is called thrashing, where most of your time is spent in page faults. The solution is often to start randomly killing processes using the most memory. 

### Swapping
The kernel can organize regions of VM as segments according to how they are swapped. This improves locality as data that gets swapped to the same file all goes to the same segment. Multiple memory areas can get swapped to the same file or partition, but anywhere in that partition. 
The swap daemon maintains a swap map that includes which blocks on disk are in use and which virtual pages are stored in said blocks. 

### Mapping
The kernel can use mapping to organize file permissions, such as allowing execute permissions through an offset. This also works for sharing if the kernel maps two processes to the same location that maps back to one file on disk. 

## Concurrency
Moore's law is dead, and now we just pile more cores into the same processor. This means we need a way to allow applications to use multiple cores at the same time