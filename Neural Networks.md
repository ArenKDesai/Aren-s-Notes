#MIT #DeepLearning #ML 

Neural networks (NNs) are made up of layers of perceptrons which map an input $x$ to an output $y$ with a linear function and a non-linear activation after. The number of layers between the input and output, called **hidden layers**, is variable. 
NNs use a **loss function** to determine how well the NN is doing at prediction. This is typically **Mean Squared Error** (MSE), but can be many other options. 
NNs use a **stochastic gradient descent** algorithm to optimize the perceptron functions. This uses calculus to determine the direction to move the parameters that would minimize the loss function. 
NNs also rely on a **learning rate**, which dictates the rate at which the perceptron functions update. This is variable. 
